{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation system\n",
    "Recommender systems are one of the most successful and widespread application of machine learning technologies in business.\n",
    "You can find large scale recommender systems in retail, video on demand, or music streaming.\n",
    "- ***Examples of recommendation systems are:***\n",
    "    1. Offering news articles to on-line newspaper readers, based on a prediction\n",
    "of reader interests.\n",
    "    2. Offering customers of an on-line retailer suggestions about what they\n",
    "might like to buy, based on their past history of purchases and/or product\n",
    "searches.\n",
    "    3. Recommending movies to user based on there previous watch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://md.ekstrandom.net/talks/2014/txstate-recsys-research/movielens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "- Exploratory Data Analysis(EDA)\n",
    "- Content based filtering\n",
    "- Collaborative Filtering\n",
    "    - Memory based collaborative filtering\n",
    "        - User-Item Filtering\n",
    "        - Item-Item Filtering\n",
    "    - Model based collaborative filtering\n",
    "        - Single Value Decomposition(SVD)\n",
    "        - SVD++\n",
    "- Evaluating Collaborative Filtering using SVD\n",
    "- Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : Movielens\n",
    "https://grouplens.org/datasets/movielens/100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep=',', encoding='latin-1', usecols=['userId','movieId','rating','timestamp'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep=',', encoding='latin-1', usecols=['movieId','title','genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = movies \n",
    "df_ratings = ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular genres of movie released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "generlist = df_movies['genres'].apply(lambda generlist_movie : str(generlist_movie).split(\"|\"))\n",
    "geners_count = {}\n",
    "\n",
    "for generlist_movie in generlist:\n",
    "    for gener in generlist_movie:\n",
    "        if(geners_count.get(gener,False)):\n",
    "            geners_count[gener]=geners_count[gener]+1\n",
    "        else:\n",
    "            geners_count[gener] = 1       \n",
    "geners_count.pop(\"(no genres listed)\")\n",
    "plt.bar(geners_count.keys(),geners_count.values(),color='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of users rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_ratings[\"rating\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of frames: \\n\"+ \" Rating DataFrame\"+ str(df_ratings.shape)+\"\\n Movies DataFrame\"+ str(df_movies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ratings_movies = pd.merge(df_movies, df_ratings, on='movieId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ratings_movies.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ratings_movies = merge_ratings_movies.drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ratings_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the rating based on user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_users = merge_ratings_movies.groupby('userId').agg([np.size, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_users = ratings_grouped_by_users.drop('movieId', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 users who have rated most of the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_users['rating']['size'].sort_values(ascending=False).head(10).plot(kind='bar', figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies = merge_ratings_movies.groupby('movieId').agg([np.mean, np.size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies = ratings_grouped_by_movies.drop('userId', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies with high average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies['rating']['mean'].sort_values(ascending=False).head(10).plot(kind='barh', figsize=(7,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies with low average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rated_movies_filter = ratings_grouped_by_movies['rating']['mean']< 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rated_movies = ratings_grouped_by_movies[low_rated_movies_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_rated_movies.head(20).plot(kind='barh', figsize=(7,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rated_movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering\n",
    "The concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used in information retrieval systems and also content based filtering mechanisms (such as a content based recommender). They are used to determine the relative importance of a document / article / news item / movie etc.\n",
    "\n",
    "### Term Frequency (TF) and Inverse Document Frequency (IDF)\n",
    "TF is simply the frequency of a word in a document. IDF is the inverse of the document frequency among the whole corpus of documents. TF-IDF is used mainly because of two reasons: Suppose we search for “the rise of analytics” on Google. It is certain that “the” will occur more frequently than “analytics” but the relative importance of analytics is higher than the search query point of view. In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).\n",
    "\n",
    "We will consider genres as an important parameter to recommend user the movie he watches based on generes of movie user has already watched.\n",
    "\n",
    "![](https://mungingdata.files.wordpress.com/2017/11/equation.png?w=430&h=336)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating distances, many similarity coefficients can be calculated. Most widely used similarity coefficients are Euclidean, Cosine, Pearson Correlation etc.\n",
    "\n",
    "**Cosine similarity**\n",
    "is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\n",
    "Given two vectors of attributes, A and B, the cosine similarity, cos(θ), is represented using a dot product and magnitude as\n",
    "Inline-style: \n",
    "![alt text](https://alexn.org/assets/img/cosine-similarity.png)\n",
    "\n",
    "We will use cosine distance here. Here we are insterested in similarity. That means higher the value more similar they are. But as the function gives us the distance, we will deduct it from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TF-IDF Vectorizer Object.\n",
    "tfidf_movies_genres = TfidfVectorizer(token_pattern = '[a-zA-Z0-9\\-]+')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "df_movies['genres'] = df_movies['genres'].replace(to_replace=\"(no genres listed)\", value=\"\")\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_movies_genres_matrix = tfidf_movies_genres.fit_transform(df_movies['genres'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_movies_genres_matrix.toarray(), columns=tfidf_movies_genres.get_feature_names_out())\n",
    "print(tfidf_df[0:10])\n",
    "# Compute the cosine similarity matrix\n",
    "# print(tfidf_movies_genres_matrix.shape)\n",
    "# print(tfidf_movies_genres_matrix.dtype)\n",
    "cosine_sim_movies = linear_kernel(tfidf_movies_genres_matrix, tfidf_movies_genres_matrix)\n",
    "#print(cosine_sim_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_based_on_genres(movie_title, cosine_sim_movies=cosine_sim_movies):\n",
    "    \"\"\"\n",
    "    Calculates top 2 movies to recommend based on given movie titles genres. \n",
    "    :param movie_title: title of movie to be taken for base of recommendation\n",
    "    :param cosine_sim_movies: cosine similarity between movies \n",
    "    :return: Titles of movies recommended to user\n",
    "    \"\"\"\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx_movie = df_movies.loc[df_movies['title'].isin([movie_title])]\n",
    "    idx_movie = idx_movie.index\n",
    "    \n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores_movies = list(enumerate(cosine_sim_movies[idx_movie][0]))\n",
    "    \n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores_movies = sorted(sim_scores_movies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores_movies = sim_scores_movies[1:3]\n",
    "    \n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores_movies]\n",
    "    \n",
    "    # Return the top 2 most similar movies\n",
    "    return df_movies['title'].iloc[movie_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_based_on_genres(\"Father of the Bride Part II (1995)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_content_model(userId):\n",
    "    \"\"\"\n",
    "    Calculates top movies to be recommended to user based on movie user has watched.  \n",
    "    :param userId: userid of user\n",
    "    :return: Titles of movies recommended to user\n",
    "    \"\"\"\n",
    "    recommended_movie_list = []\n",
    "    movie_list = []\n",
    "    df_rating_filtered = df_ratings[df_ratings[\"userId\"]== userId]\n",
    "    for key, row in df_rating_filtered.iterrows():\n",
    "        movie_list.append((df_movies[\"title\"][row[\"movieId\"]==df_movies[\"movieId\"]]).values) \n",
    "    for index, movie in enumerate(movie_list):\n",
    "        for key, movie_recommended in get_recommendations_based_on_genres(movie[0]).iteritems():\n",
    "            recommended_movie_list.append(movie_recommended)\n",
    "\n",
    "    # removing already watched movie from recommended list    \n",
    "    for movie_title in recommended_movie_list:\n",
    "        if movie_title in movie_list:\n",
    "            recommended_movie_list.remove(movie_title)\n",
    "    \n",
    "    return set(recommended_movie_list)\n",
    "get_recommendation_content_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with KNN\n",
    "Here the model is evaluated on based of if there is exact match of genres with the genres of movie which is already watch by user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_movies.iloc[:30,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "def get_movie_label(movie_id):\n",
    "    \"\"\"\n",
    "    Get the cluster label to which movie belongs by KNN algorithm.  \n",
    "    :param movie_id: movie id\n",
    "    :return: genres label to movie belong\n",
    "    \"\"\"\n",
    "    classifier = KNeighborsClassifier(n_neighbors=4)\n",
    "    x= tfidf_movies_genres_matrix\n",
    "    y = df_movies.iloc[:,-1]\n",
    "    classifier.fit(x, y)\n",
    "    y_pred = classifier.predict(tfidf_movies_genres_matrix[movie_id])\n",
    "    return y_pred\n",
    "print(get_movie_label(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = 0  \n",
    "false_count = 0  \n",
    "\n",
    "def evaluate_content_based_model():  \n",
    "    \"\"\"  \n",
    "    Evaluate content-based model.  \n",
    "    \"\"\"  \n",
    "    global true_count, false_count  \n",
    "    for index, row in df_movies.iterrows():  \n",
    "        movies_recommended_by_model = get_recommendations_based_on_genres(row[\"title\"])  \n",
    "        predicted_genres = get_movie_label(movies_recommended_by_model.index)  \n",
    "        \n",
    "        # Assuming row[\"genres\"] is a string containing multiple genres, split them into a set  \n",
    "        actual_genres = set(row[\"genres\"].split('|'))  # Change delimiter if needed  \n",
    "\n",
    "        # Check if any predicted genre matches with actual genres  \n",
    "        if any(predicted_genre in actual_genres for predicted_genre in predicted_genres):  \n",
    "            true_count += 1  \n",
    "        else:  \n",
    "            false_count += 1  \n",
    "\n",
    "# Call the evaluate function  \n",
    "evaluate_content_based_model()  \n",
    "\n",
    "# Calculate total and handle potential division by zero  \n",
    "total = true_count + false_count  \n",
    "if total > 0:  \n",
    "    print(\"Hit: \" + str(true_count / total))  \n",
    "    print(\"Fault: \" + str(false_count / total))  \n",
    "else:  \n",
    "    print(\"No predictions made; cannot calculate hit or fault rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = 0\n",
    "false_count = 0\n",
    "def evaluate_content_based_model():\n",
    "    \"\"\"\n",
    "    Evaluate content based model.  \n",
    "    \"\"\"\n",
    "    for key, colums in df_movies.iterrows():\n",
    "        movies_recommended_by_model = get_recommendations_based_on_genres(colums[\"title\"])\n",
    "        predicted_genres  = get_movie_label(movies_recommended_by_model.index)\n",
    "        for predicted_genre in predicted_genres:\n",
    "            global true_count, false_count\n",
    "            if predicted_genre == colums[\"genres\"]:\n",
    "                true_count = true_count+1\n",
    "            else:\n",
    "#                 print(colums[\"genres\"])\n",
    "#                 print(predicted_genre)\n",
    "                false_count = false_count +1\n",
    "evaluate_content_based_model()\n",
    "total = true_count + false_count\n",
    "print(\"Hit:\"+ str(true_count/total))\n",
    "print(\"Fault:\" + str(false_count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = movies \n",
    "df_ratings = ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "Types of collaborative filtering techniques\n",
    "* Memory based \n",
    " - User-Item Filtering\n",
    " - Item-Item Filtering\n",
    "* Model based \n",
    " - Matrix Factorization\n",
    " - Clustering\n",
    " - Deep Learning\n",
    " ![alt text](https://cdn-images-1.medium.com/max/1500/1*7uW5hLXztSu_FOmZOWpB6g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Based Approach\n",
    " ![alt text](https://cdn-images-1.medium.com/max/1500/1*QvhetbRjCr1vryTch_2HZQ.jpeg)\n",
    "\n",
    "In either scenario, we builds a similarity matrix. For user-user collaborative filtering, the user-similarity matrix will consist of some distance metrics that measure the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.\n",
    "\n",
    "There are 3 distance similarity metrics that are usually used in collaborative filtering:\n",
    "\n",
    "- **Jaccard Similarity**\n",
    "- **Cosine Similarity** \n",
    "- **Pearson Similarity** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Item Filtering\n",
    "Item-item collaborative filtering, or item-based, or item-to-item, is a form of collaborative filtering for recommender systems based on the similarity between items calculated using people's ratings of those items.\n",
    "\n",
    "\n",
    "Item-item collaborative filtering was invented and used by Amazon.com\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*skK2fqWiBF7weHU8SjuCzw.png)\n",
    "ITEM-ITEM collaborative filtering look for items that are similar to the articles that user has already rated and recommend most similar articles. But what does that mean when we say item-item similarity? In this case we don’t mean whether two items are the same by attribute like Fountain pen and pilot pen are similar because both are pen. Instead, what similarity means is how people treat two items the same in terms of like and dislike.\n",
    "\n",
    "It is quite similar to previous algorithm, but instead of finding user’s look-alike, we try finding movie’s look-alike. Once we have movie’s look-alike matrix, we can easily recommend alike movies to user who have rated any movie from the dataset. This algorithm is far less resource consuming than user-user collaborative filtering. Hence, for a new user, the algorithm takes far lesser time than user-user collaborate as we don’t need all similarity scores between users. And with fixed number of movies, movie-movie look alike matrix is fixed over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Item-Item Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_ratings=pd.merge(df_movies, df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Pivot table function is used as we want one to one maping between movies, user and their rating. \n",
    "So by default pivot_table command takes average if we have multiple values of one combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_items = df_movies_ratings.pivot_table(index=['movieId'],columns=['userId'],values='rating').reset_index(drop=True)\n",
    "ratings_matrix_items.fillna( 0, inplace = True )\n",
    "ratings_matrix_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarity = 1 - pairwise_distances( ratings_matrix_items.as_matrix(), metric=\"cosine\" )\n",
    "np.fill_diagonal( movie_similarity, 0 ) #Filling diagonals with 0s for future use when sorting is done\n",
    "ratings_matrix_items = pd.DataFrame( movie_similarity )\n",
    "ratings_matrix_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function will take the movie name as a input and will find the movies which are similar to this movie.\n",
    "This function first find the index of movie in movies frame and then take the similarity of movie and align in movies dataframe so that we can get the similarity of the movie with all other movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_similarity(movieName): \n",
    "    \"\"\"\n",
    "    recomendates similar movies\n",
    "   :param data: name of the movie \n",
    "   \"\"\"\n",
    "    try:\n",
    "        #user_inp=input('Enter the reference movie title based on which recommendations are to be made: ')\n",
    "        user_inp=movieName\n",
    "        inp=df_movies[df_movies['title']==user_inp].index.tolist()\n",
    "        inp=inp[0]\n",
    "\n",
    "        df_movies['similarity'] = ratings_matrix_items.iloc[inp]\n",
    "        df_movies.columns = ['movie_id', 'title', 'release_date','similarity']\n",
    "    except:\n",
    "        print(\"Sorry, the movie is not in the database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the user id of the user for which we have to recommend movies.\n",
    "Then we find the movies which are rated 5 or 4.5 by the user for whom we want to recommend movies.\n",
    "We are finding this because as we know that in Item-Item similarity approach we recommended movies to the user based on his previous selection.\n",
    "So to foster our algorithm we are finding movies which are liked by the user most and on bases of that we will recommend movies with are similar to movies highly rated by the user.\n",
    "Then our function has appended the similarity of the movie highly rated by the user to our movies data frame.\n",
    "Now we will sort the frame as per the similarity in descending order so that we can get the movies which are highly similar to movie highly rated bu our customer.\n",
    "Now we filter the movies which are most similar as per the similarity so if similarity is greater than 0.45 then we are considering the movies.\n",
    "Now the function goes ahead and see which all movies user has seen and then filter out the movies which he has not seen and than recommended that movies to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendedMoviesAsperItemSimilarity(user_id):\n",
    "    \"\"\"\n",
    "     Recommending movie which user hasn't watched as per Item Similarity\n",
    "    :param user_id: user_id to whom movie needs to be recommended\n",
    "    :return: movieIds to user \n",
    "    \"\"\"\n",
    "    user_movie= df_movies_ratings[(df_movies_ratings.userId==user_id) & df_movies_ratings.rating.isin([5,4.5])][['title']]\n",
    "    user_movie=user_movie.iloc[0,0]\n",
    "    item_similarity(user_movie)\n",
    "    sorted_movies_as_per_userChoice=df_movies.sort_values( [\"similarity\"], ascending = False )\n",
    "    print(sorted_movies_as_per_userChoice)\n",
    "\"\"\"\n",
    "    sorted_movies_as_per_userChoice=sorted_movies_as_per_userChoice[sorted_movies_as_per_userChoice['similarity'] >=0.45]['movie_id']\n",
    "    recommended_movies=list()\n",
    "    df_recommended_item=pd.DataFrame()\n",
    "    user2Movies= df_ratings[df_ratings['userId']== user_id]['movieId']\n",
    "    for movieId in sorted_movies_as_per_userChoice:\n",
    "            if movieId not in user2Movies:\n",
    "                df_new= df_ratings[(df_ratings.movieId==movieId)]\n",
    "                df_recommended_item=pd.concat([df_recommended_item,df_new])\n",
    "            best10=df_recommended_item.sort_values([\"rating\"], ascending = False )[1:10] \n",
    "    return best10['movieId']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieIdToTitle(listMovieIDs):\n",
    "    \"\"\"\n",
    "     Converting movieId to titles\n",
    "    :param user_id: List of movies\n",
    "    :return: movie titles\n",
    "    \"\"\"\n",
    "    movie_titles= list()\n",
    "    for id in listMovieIDs:\n",
    "        movie_titles.append(df_movies[df_movies['movie_id']==id]['title'])\n",
    "    return movie_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id=50\n",
    "print(\"Recommended movies,:\\n\",movieIdToTitle(recommendedMoviesAsperItemSimilarity(user_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item Filtering\n",
    "The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.\n",
    "![alt text](https://cdn-images-1.medium.com/max/1500/0*o0zVW2O6Rv-LI5Mu.png)\n",
    "\n",
    "Here we find look alike users based on similarity and recommend movies which first user’s look-alike has chosen in past. This algorithm is very effective but takes a lot of time and resources. It requires to compute every user pair information which takes time. Therefore, for big base platforms, this algorithm is hard to implement without a very strong parallelizable system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of User-Item Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In similar way as we did for ItemItem similarity we will create a matrix but here we will keep rows as user and columns as movieId as we want a vector of different users.\n",
    "Then in similar ways we will find distance and similarity between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_users = df_movies_ratings.pivot_table(index=['userId'],columns=['movieId'],values='rating').reset_index(drop=True)\n",
    "ratings_matrix_users.fillna( 0, inplace = True )\n",
    "movie_similarity = 1 - pairwise_distances( ratings_matrix_users.as_matrix(), metric=\"cosine\" )\n",
    "np.fill_diagonal( movie_similarity, 0 ) #Filling diagonals with 0s for future use when sorting is done\n",
    "ratings_matrix_users = pd.DataFrame( movie_similarity )\n",
    "ratings_matrix_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here now we have similarity of users in colums with respective users in row. So if we find maximum value in a column we will get the user with highest similarity. So now we can have a pair of users which are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_users.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_users.idxmax(axis=1).sample( 10, random_state = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_user_series= ratings_matrix_users.idxmax(axis=1)\n",
    "df_similar_user= similar_user_series.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similar_user.columns=['similarUser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similar_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function takes id of the user to whom we have to recommend movies. On basis of that, we find the user which is similar to that user and then filter the movies which are highly rated by the user to recommend them to given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_recommended=list()\n",
    "def getRecommendedMoviesAsperUserSimilarity(userId):\n",
    "    \"\"\"\n",
    "     Recommending movies which user hasn't watched as per User Similarity\n",
    "    :param user_id: user_id to whom movie needs to be recommended\n",
    "    :return: movieIds to user \n",
    "    \"\"\"\n",
    "    user2Movies= df_ratings[df_ratings['userId']== userId]['movieId']\n",
    "    sim_user=df_similar_user.iloc[0,0]\n",
    "    df_recommended=pd.DataFrame(columns=['movieId','title','genres','userId','rating','timestamp'])\n",
    "    for movieId in df_ratings[df_ratings['userId']== sim_user]['movieId']:\n",
    "        if movieId not in user2Movies:\n",
    "            df_new= df_movies_ratings[(df_movies_ratings.userId==sim_user) & (df_movies_ratings.movieId==movieId)]\n",
    "            df_recommended=pd.concat([df_recommended,df_new])\n",
    "        best10=df_recommended.sort_values(['rating'], ascending = False )[1:10]  \n",
    "    return best10['movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id=50\n",
    "recommend_movies= movieIdToTitle(getRecommendedMoviesAsperUserSimilarity(user_id))\n",
    "print(\"Movies you should watch are:\\n\")\n",
    "print(recommend_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_similar_movies( user1, user2 ):\n",
    "    \n",
    "    \"\"\"\n",
    "     Returning common movies and ratings of same for both the users\n",
    "    :param user1,user2: user ids of 2 users need to compare\n",
    "    :return: movieIds to user \n",
    "    \"\"\"\n",
    "    common_movies = df_movies_ratings[df_movies_ratings.userId == user1].merge(\n",
    "      df_movies_ratings[df_movies_ratings.userId == user2],\n",
    "      on = \"movieId\",\n",
    "      how = \"inner\" )\n",
    "    common_movies.drop(['movieId','genres_x','genres_y', 'timestamp_x','timestamp_y','title_y'],axis=1,inplace=True)\n",
    "    return common_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_similar_movies(587,511)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of two methods\n",
    "\n",
    "Challenges with User similarity\n",
    "- The challenge with calculating user similarity is the user need to have some prior purchases and should have rated them.\n",
    "- This recommendation technique does not work for new users.\n",
    "- The system need to wait until the user make some purchases and rates them. Only then similar users can be found and recommendations can be made. This is called cold start problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition is a method of decomposing a matrix into three other matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*i4rDKIAE0o1ZXtBd.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  (1)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "    A is an m × n matrix\n",
    "    U is an m × r orthogonal matrix\n",
    "    S is an r × r diagonal matrix\n",
    "    V is an r × n orthogonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*mpQRslOtBoqfcdP7.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we’ve collapsed the diagonal matrix, S, into a vector, thus simplifying the expression into a single summation. The variables, {sᵢ}, are called singular values and are normally arranged from largest to smallest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*7J7tN93BocfX5ZbA.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of U are called left singular vectors, while those of V are called right singular vectors.\n",
    "\n",
    "We know that U and V are orthogonal, that is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*SHyyI8TOYFxCbLbw.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where I is the identity matrix. Only the diagonals of the identity matrix are 1, with all other values being 0. Note that because U is not square we cannot say that U Transpose(U)=I, so U is only orthogonal in one direction.\n",
    "\n",
    "Using the orthogonality property, we can rearrange (1) into the following pair of eigenvalue equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reduction\n",
    "A typical machine learning problem might have several hundred or more variables, while many machine learning algorithms will break down if presented with more than a few dozen. This makes singular value decomposition indispensable in ML for variable reduction.\n",
    "\n",
    "We have already seen in Equation (6) how an SVD with a reduced number of singular values can closely approximate a matrix. This can be used for data compression by storing the truncated forms of U, S, and V in place of A and for variable reduction by replacing A with U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-based collaborative filtering approaches that compute distance relationships between items or users have these two major issues:\n",
    "\n",
    "* It doesn’t scale particularly well to massive datasets, especially for real-time recommendations based on user behavior similarities — which takes a lot of computations.\n",
    "* Ratings matrices may be overfitting to noisy representations of user tastes and preferences. When we use distance based “neighborhood” approaches on raw data, we match to sparse low-level details that we assume represent the user’s preference vector instead of the vector itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*bWAAyciKq580ArTix_O3nw.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we need to apply Dimensionality Reduction technique to derive the tastes and preferences from the raw data, otherwise known as doing <b>low-rank matrix factorization</b>.\n",
    "\n",
    "   * We can discover hidden correlations / features in the raw data.\n",
    "   * We can remove redundant and noisy features that are not useful.\n",
    "   * We can interpret and visualize the data easier.\n",
    "   * We can also access easier data storage and processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try out the same in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep=',', encoding='latin-1', usecols=['userId','movieId','rating','timestamp'])\n",
    "\n",
    "# Reading users file\n",
    "#users = pd.read_csv('users.dat', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep=',', encoding='latin-1', usecols=['movieId','title','genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings.userId.unique().shape[0]\n",
    "n_movies = ratings.movieId.unique().shape[0]\n",
    "print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the format of our ratings matrix to be one row per user and one column per movie. To do so, we willl pivot ratings to get that and call the new variable Ratings (with a capital *R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratings = ratings.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)\n",
    "Ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we need to de-normalize the data (normalize by each users mean) and convert it from a dataframe to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "R = Ratings.as_matrix()\n",
    "#print(R)\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "#print(user_ratings_mean.shape)\n",
    "print(user_ratings_mean.size)\n",
    "Ratings_demeaned = R - user_ratings_mean.reshape(-1, 1) ## Making the user_ratings_mean vertical by reshaping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our ratings matrix properly formatted and normalized, we are ready to do some dimensionality reduction. But first, let's go over the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-based Collaborative Filtering</b> is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF:\n",
    "\n",
    "   * The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items.\n",
    "   * When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization, you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector.\n",
    "   * You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix.\n",
    "\n",
    "For example, let's check the sparsity of the ratings dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = round(1.0 - len(ratings) / float(n_users * n_movies), 3)\n",
    "print('The sparsity level of MovieLens100K dataset is ' +  str(sparsity * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well-known matrix factorization method is Singular value decomposition (SVD). At a high level, SVD is an algorithm that decomposes a matrix $A$ into the best lower rank (i.e. smaller/simpler) approximation of the original matrix $A$. Mathematically, it decomposes A into a two unitary matrices and a diagonal matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*W4MnB2hyvgqedLmwJLrpqw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $A$ is the input data matrix (users's ratings), $U$ is the left singular vectors (user \"features\" matrix), $\\Sigma$ is the diagonal matrix of singular values (essentially weights/strengths of each concept), and  $V^{T}$ is the right singluar vectors (movie \"features\" matrix). $U$ and $V^{T}$ are column orthonomal, and represent different things. $U$ represents how much users \"like\" each feature and $V^{T}$ represents how relevant each feature is to each movie.\n",
    "\n",
    "To get the lower rank approximation, I take these matrices and keep only the top $k$ features, which can be thought of as the underlying tastes and preferences vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy and Numpy both have functions to do the singular value decomposition. We are going to use the Scipy function svds because it let’s us choose how many latent factors we want to use to approximate the original ratings matrix (instead of having to truncate it after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, Vt = svds(Ratings_demeaned, k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of sigma: ' , sigma.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to leverage matrix multiplication to get predictions, we willl convert the $\\Sigma$ (now are values) to the diagonal matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Shape of sigma: ', sigma.shape)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of U: ', U.shape)\n",
    "print('Shape of Vt: ', Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions from the Decomposed Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to make movie ratings predictions for every user. We can do it all at once by following the math and matrix multiply $U$, $\\Sigma$, and $V^{T}$ back to get the rank $k=50$ approximation of $A$.\n",
    "\n",
    "But first, we need to add the user means back to get the actual star ratings prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('All user predicted rating : ', all_user_predicted_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the predictions matrix for every user, we can build a function to recommend movies for any user. We return the list of movies the user has already rated, for the sake of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the column names from the ratings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Rating Dataframe column names', Ratings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(all_user_predicted_ratings, columns = Ratings.columns)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to return the movies with the highest predicted rating that the specified user hasn't already rated. Though we didn't use any explicit movie content features (such as genre or title), we will merge in that information to get a more complete picture of the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(predictions, userID, movies, original_ratings, num_recommendations):\n",
    "    \"\"\"\n",
    "    Implementation of SVD by hand\n",
    "    :param predictions : The SVD reconstructed matrix, \n",
    "    userID : UserId for which you want to predict the top rated movies, \n",
    "    movies : Matrix with movie data, original_ratings : Original Rating matrix, \n",
    "    num_recommendations : num of recos to be returned\n",
    "    :return: num_recommendations top movies\n",
    "    \"\"\" \n",
    "    # Get and sort the user's predictions\n",
    "    user_row_number = userID - 1 # User ID starts at 1, not 0\n",
    "    sorted_user_predictions = predictions.iloc[user_row_number].sort_values(ascending=False) # User ID starts at 1\n",
    "    \n",
    "    # Get the user's data and merge in the movie information.\n",
    "    user_data = original_ratings[original_ratings.userId == (userID)]\n",
    "    user_full = (user_data.merge(movies, how = 'left', left_on = 'movieId', right_on = 'movieId').\n",
    "                     sort_values(['rating'], ascending=False)\n",
    "                 )\n",
    "\n",
    "    print('User {0} has already rated {1} movies.'.format(userID, user_full.shape[0]))\n",
    "    print('Recommending highest {0} predicted ratings movies not already rated.'.format(num_recommendations))\n",
    "    \n",
    "    # Recommend the highest predicted rating movies that the user hasn't seen yet.\n",
    "    recommendations = (movies[~movies['movieId'].isin(user_full['movieId'])].\n",
    "         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n",
    "               left_on = 'movieId',\n",
    "               right_on = 'movieId').\n",
    "         rename(columns = {user_row_number: 'Predictions'}).\n",
    "         sort_values('Predictions', ascending = False).\n",
    "                       iloc[:num_recommendations, :-1]\n",
    "                      )\n",
    "\n",
    "    return user_full, recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to recommend 20 movies for user with ID 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "already_rated, predictions = recommend_movies(preds, 150, movies, ratings, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 20 movies that User 1310 has rated \n",
    "already_rated.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 20 movies that User 1310 hopefully will enjoy\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like pretty good recommendations. It's good to see that, although we didn't actually use the genre of the movie as a feature, the truncated matrix factorization features \"picked up\" on the underlying tastes and preferences of the user. We have recommended some Action, Adventure, Romance, Thriller movies - all of which were genres of some of this user's top rated movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't forget to evaluate our model, can we?\n",
    "\n",
    "Instead of doing manually like the last time, we will use the Surprise library that provided various ready-to-use powerful prediction algorithms including (SVD) to evaluate its RMSE (Root Mean Squared Error) on the MovieLens dataset. It is a Python scikit building and analyzing recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries from Surprise package\n",
    "from surprise import Reader, Dataset, SVD, evaluate\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load Reader library\n",
    "reader = Reader()\n",
    "\n",
    "# Load ratings dataset with Dataset library\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split the dataset for 5-fold evaluation\n",
    "data.split(n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the SVD algorithm.\n",
    "svd = SVD()\n",
    "\n",
    "# Compute the RMSE of the SVD algorithm.\n",
    "evaluate(svd, data, measures=['RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a mean Root Mean Square Error of  0.87 which is pretty good. Let's now train on the dataset and arrive at predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "svd.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick again user with ID 150 and check the ratings he has given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings[ratings['userId'] == 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use SVD to predict the rating that User with ID 150 will give to a random movie (let's say with Movie ID 1994)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svd.predict(150, 1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For movie with ID 1994, I get an estimated prediction of 3.18210. The recommender system works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have predicted the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.predict(150, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.predict(150, 194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd.predict(150, 190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "from surprise import SVDpp\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use movielens-100K\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset, testset = train_test_split(data, test_size=.15)\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD++\n",
    "To build a robust recommender system, we need to develop models which factor in both explicit and implicit user feedback. For our Movielens dataset, a less obvious kind of implicit data does exist. The dataset does not only tell us the rating values, but also which movies users rate, regardless of how they rated these movies. In other words, a user implicitly tells us about her preferences by choosing to voice her opinion and vote a (high or low) rating. This reduces the ratings matrix into a binary matrix, where “1” stands for “rated”, and “0” for “not rated”. Admittedly, this binary data is not as vast and independent as other sources of implicit feedback could be. Nonetheless, we have found that incorporating this kind of implicit data – which inherently exist in every rating based recommender system – significantly improves prediction accuracy. SVD++ factors in this implicit feedback and gives better accuracy as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_svdpp = SVDpp(n_factors=160, n_epochs=10, lr_all=0.005, reg_all=0.1)\n",
    "algo_svdpp.fit(trainset)\n",
    "test_pred = algo_svdpp.test(testset)\n",
    "print(\"SVDpp : Test Set\")\n",
    "accuracy.rmse(test_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have explored and used Surprise package:\n",
    "This package has been specially developed to make recommendation based on collaborative filtering easy. It has default implementation for a variety of CF algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hit Ratio***\n",
    "It is ratio of number of hits/ Total recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_collaborative_svd_model(userId,userOrItem):\n",
    "    \"\"\"\n",
    "    hydrid the functionality of Collaborative based and svd based model to see if ratings of predicted movies \n",
    "    :param userId: userId of user, userOrItem is a boolean value if True it is User-User and if false Item-Item\n",
    "    :return: dataframe of movies and ratings\n",
    "    \"\"\" \n",
    "    movieIdsList= list()\n",
    "    movieRatingList=list()\n",
    "    movieIdRating= pd.DataFrame(columns=['movieId','rating'])\n",
    "    if userOrItem== True:\n",
    "        movieIdsList=getRecommendedMoviesAsperUserSimilarity(userId)\n",
    "    else:\n",
    "        movieIdsList=recommendedMoviesAsperItemSimilarity(user_id)\n",
    "    for movieId in movieIdsList:\n",
    "        predict = svd.predict(userId, movieId)\n",
    "        movieRatingList.append([movieId,predict.est])\n",
    "        movieIdRating = pd.DataFrame(np.array(movieRatingList), columns=['movieId','rating'])\n",
    "        count=movieIdRating[(movieIdRating['rating'])>=3]['movieId'].count()\n",
    "        total=movieIdRating.shape[0]\n",
    "        hit_ratio= count/total\n",
    "    return hit_ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Hit ratio of User-user collaborative filtering\")\n",
    "print(evaluation_collaborative_svd_model(user_id,True))\n",
    "print(\"Hit ratio of Item-Item collaborative filtering\")\n",
    "print(evaluation_collaborative_svd_model(user_id,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid model:\n",
    "\n",
    "After developing individual models as discussed earlier, we now stack them in order to get better results.\n",
    "\n",
    "Content Based Filtering + SVD\n",
    "\n",
    "Steps:\n",
    "1. Run Content based filtering and determine the movies which we want to recommend to the user.\n",
    "2. Filter and sort the recommendations of CF using SVD predicted ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies=movies\n",
    "def hybrid_content_svd_model(userId):\n",
    "    \"\"\"\n",
    "    hydrid the functionality of content based and svd based model to recommend user top 10 movies. \n",
    "    :param userId: userId of user\n",
    "    :return: list of movies recommended with rating given by svd model\n",
    "    \"\"\"\n",
    "    recommended_movies_by_content_model = get_recommendation_content_model(userId)\n",
    "    recommended_movies_by_content_model = df_movies[df_movies.apply(lambda movie: movie[\"title\"] in recommended_movies_by_content_model, axis=1)]\n",
    "    for key, columns in recommended_movies_by_content_model.iterrows():\n",
    "        predict = svd.predict(userId, columns[\"movieId\"])\n",
    "        recommended_movies_by_content_model.loc[key, \"svd_rating\"] = predict.est\n",
    "#         if(predict.est < 2):\n",
    "#             recommended_movies_by_content_model = recommended_movies_by_content_model.drop([key])\n",
    "    return recommended_movies_by_content_model.sort_values(\"svd_rating\", ascending=False).iloc[0:11]\n",
    "        \n",
    "hybrid_content_svd_model(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We implemented and evaluated different widely used Movie Recommendation models. Also, we developed our own models like the hybrid model (Content based + SVD) as discussed earlier. We evaluated each model with the appropriate evaluation metric.\n",
    "We implemented novel technique to evaluate collaborative filtering algorithm by using SVD and hit ratio as a metric\n",
    "\n",
    "We attempted to build a model-based Collaborative Filtering movie recommendation sytem based on latent features from a low rank matrix factorization method called SVD and SVD++. As it captures the underlying features driving the raw data, it can scale significantly better to massive datasets as well as make better recommendations based on user's tastes. For SVD model we get an RMSE of <b>0.87</b> and for SVD++ model we get an RMSE of <b>0.938</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "* https://www.datacamp.com/community/tutorials/recommender-systems-python\n",
    "* https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0\n",
    "* https://medium.com/@james_aka_yale/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223\n",
    "* http://www.awesomestats.in/python-recommending-movies/\n",
    "* https://medium.com/@james_aka_yale/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223\n",
    "* https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "* https://github.com/gpfvic/IRR/blob/master/Factorization%20meets%20the%20neighborhood-%20a%20multifaceted%20collaborative%20filtering%20model.pdf\n",
    "* https://surprise.readthedocs.io/en/stable/index.html\n",
    "* https://www.quora.com/Whats-the-difference-between-SVD-and-SVD++\n",
    "* https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254\n",
    "* https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
